{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YqZd3YT8EKp"
      },
      "source": [
        "### Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufMRWg9Ys-L_"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "import os\n",
        "import sys\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvrCwMES74W4"
      },
      "source": [
        "### Hardware Accelerator Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjUFclRx78qk",
        "outputId": "5abbd97b-e63c-4a56-f541-0b352d2471af"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pg-WAkTVzo2Q"
      },
      "outputs": [],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACZiV7w89N0l"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xv5JdFPh-iSs"
      },
      "outputs": [],
      "source": [
        "# defining data transforms for train and test datasets\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpYR5mrD9R99",
        "outputId": "f8183453-42b0-4694-f0c9-be25b1a26baa"
      },
      "outputs": [],
      "source": [
        "# importing CIFAR10 train and test datasets\n",
        "\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwMYna8fAYG9",
        "outputId": "8520f613-716a-462c-c87d-d977005a6494"
      },
      "outputs": [],
      "source": [
        "# number of data points in train dataset\n",
        "\n",
        "trainset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZoCxJBHyBCNH",
        "outputId": "95c07616-af06-4afb-9e62-780fb152905b"
      },
      "outputs": [],
      "source": [
        "# number of data points in test dataset\n",
        "\n",
        "testset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk708NCiBX7T"
      },
      "outputs": [],
      "source": [
        "# creating data loaders for train and test datasets\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h29ZWJm5DAuh",
        "outputId": "4eefd6ff-3251-42a5-c55d-f9d3bd178cc6"
      },
      "outputs": [],
      "source": [
        "# The different classes in CIFAR10 dataset are\n",
        "\n",
        "trainset.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EInXPW4McCbW"
      },
      "source": [
        "### ResNet18 NN Basic Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F2qgIJrKdlBR"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride = 1, conv_kernel_size = 3, skip_connection_kernel_size = 1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = conv_kernel_size, stride = stride, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = conv_kernel_size, stride = 1, padding = 1, bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, self.expansion * out_channels, kernel_size = skip_connection_kernel_size, stride = stride, bias = False),\n",
        "                nn.BatchNorm2d(self.expansion * out_channels)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q426VQXygcsk"
      },
      "source": [
        "### ResNet18 Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "P0v523cAghUa"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, channels, conv_kernel_size, skip_connection_kernel_size, avg_maxPool_size, num_classes = 10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.input = channels[0]\n",
        "        self.conv_kernel_size = conv_kernel_size\n",
        "        self.skip_connection_kernel_size = skip_connection_kernel_size\n",
        "        self.avg_maxPool_size = avg_maxPool_size\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, channels[0], kernel_size = 3, stride = 1, padding = 1, bias = False)\n",
        "        self.bn1 = nn.BatchNorm2d(channels[0])\n",
        "        self.layer1 = self._make_layer(block[0], channels[0], num_blocks[0], stride = 1)\n",
        "        self.layer2 = self._make_layer(block[1], channels[1], num_blocks[1], stride = 2)\n",
        "        self.layer3 = self._make_layer(block[2], channels[2], num_blocks[2], stride = 2)\n",
        "        self.layer4 = self._make_layer(block[3], channels[3], num_blocks[3], stride = 2)\n",
        "        self.linear = nn.Linear(channels[3] * block[3].expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.input, out_channels, stride, self.conv_kernel_size, self.skip_connection_kernel_size))\n",
        "            self.input = out_channels * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, self.avg_maxPool_size)\n",
        "        out = torch.flatten(out, 1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLeaBXhHjhwf"
      },
      "source": [
        "### Initialize the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "J7pxIGqijjz2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "            Conv2d-3           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
            "            Conv2d-5           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 32, 32]             128\n",
            "        BasicBlock-7           [-1, 64, 32, 32]               0\n",
            "            Conv2d-8           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 32, 32]             128\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "       BasicBlock-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-14          [-1, 128, 16, 16]             256\n",
            "           Conv2d-15          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-16          [-1, 128, 16, 16]             256\n",
            "           Conv2d-17          [-1, 128, 16, 16]           8,192\n",
            "      BatchNorm2d-18          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-19          [-1, 128, 16, 16]               0\n",
            "           Conv2d-20          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-21          [-1, 128, 16, 16]             256\n",
            "           Conv2d-22          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 16, 16]             256\n",
            "       BasicBlock-24          [-1, 128, 16, 16]               0\n",
            "           Conv2d-25            [-1, 232, 8, 8]         267,264\n",
            "      BatchNorm2d-26            [-1, 232, 8, 8]             464\n",
            "           Conv2d-27            [-1, 232, 8, 8]         484,416\n",
            "      BatchNorm2d-28            [-1, 232, 8, 8]             464\n",
            "           Conv2d-29            [-1, 232, 8, 8]          29,696\n",
            "      BatchNorm2d-30            [-1, 232, 8, 8]             464\n",
            "       BasicBlock-31            [-1, 232, 8, 8]               0\n",
            "           Conv2d-32            [-1, 232, 8, 8]         484,416\n",
            "      BatchNorm2d-33            [-1, 232, 8, 8]             464\n",
            "           Conv2d-34            [-1, 232, 8, 8]         484,416\n",
            "      BatchNorm2d-35            [-1, 232, 8, 8]             464\n",
            "       BasicBlock-36            [-1, 232, 8, 8]               0\n",
            "           Conv2d-37            [-1, 268, 4, 4]         559,584\n",
            "      BatchNorm2d-38            [-1, 268, 4, 4]             536\n",
            "           Conv2d-39            [-1, 268, 4, 4]         646,416\n",
            "      BatchNorm2d-40            [-1, 268, 4, 4]             536\n",
            "           Conv2d-41            [-1, 268, 4, 4]          62,176\n",
            "      BatchNorm2d-42            [-1, 268, 4, 4]             536\n",
            "       BasicBlock-43            [-1, 268, 4, 4]               0\n",
            "           Conv2d-44            [-1, 268, 4, 4]         646,416\n",
            "      BatchNorm2d-45            [-1, 268, 4, 4]             536\n",
            "           Conv2d-46            [-1, 268, 4, 4]         646,416\n",
            "      BatchNorm2d-47            [-1, 268, 4, 4]             536\n",
            "       BasicBlock-48            [-1, 268, 4, 4]               0\n",
            "           Linear-49                   [-1, 10]           2,690\n",
            "================================================================\n",
            "Total params: 4,994,298\n",
            "Trainable params: 4,994,298\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 10.75\n",
            "Params size (MB): 19.05\n",
            "Estimated Total Size (MB): 29.82\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "net = ResNet([BasicBlock, BasicBlock, BasicBlock, BasicBlock], [2, 2, 2, 2], [64, 128, 232, 268], 3, 1, 4)  # Reaches ~80% within 5 epochs\n",
        "summary(net, input_size=(3,32,32))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvhKWuXTjzEZ",
        "outputId": "65d0a6ed-b8bb-475e-fb9e-0a74bf4e2d88"
      },
      "outputs": [],
      "source": [
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n",
        "    print(\"Device is set to CUDA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtC0jvf_kQYU"
      },
      "outputs": [],
      "source": [
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0 # start from epoch 0 or last checkpoint epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcyhvqZfkuTh"
      },
      "source": [
        "### Define Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tY8PnX79kx6f"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjZwX5IGlC6v"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mH6UrxulEb7"
      },
      "outputs": [],
      "source": [
        "# def train(epoch):\n",
        "#     print('\\nEpoch: %d' % epoch)\n",
        "#     net.train()\n",
        "#     train_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "#         inputs, targets = inputs.to(device), targets.to(device)\n",
        "#         optimizer.zero_grad()\n",
        "#         outputs = net(inputs)\n",
        "#         loss = criterion(outputs, targets)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         train_loss += loss.item()\n",
        "#         _, predicted = outputs.max(1)\n",
        "#         total += targets.size(0)\n",
        "#         correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#     print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'% (train_loss/(batch_idx+1), 100.*correct/total, correct, total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlaAGeTrlGZe"
      },
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfc1pPlclHu1"
      },
      "outputs": [],
      "source": [
        "# def test(epoch):\n",
        "#     global best_acc\n",
        "#     net.eval()\n",
        "#     test_loss = 0\n",
        "#     correct = 0\n",
        "#     total = 0\n",
        "#     with torch.no_grad():\n",
        "#         for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "#             inputs, targets = inputs.to(device), targets.to(device)\n",
        "#             outputs = net(inputs)\n",
        "#             loss = criterion(outputs, targets)\n",
        "\n",
        "#             test_loss += loss.item()\n",
        "#             _, predicted = outputs.max(1)\n",
        "#             total += targets.size(0)\n",
        "#             correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "#         print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'% (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "#     # Save checkpoint.\n",
        "#     acc = 100.*correct/total\n",
        "#     if acc > best_acc:\n",
        "#         print('Saving..')\n",
        "#         state = {\n",
        "#             'net': net.state_dict(),\n",
        "#             'acc': acc,\n",
        "#             'epoch': epoch,\n",
        "#         }\n",
        "#         if not os.path.isdir('checkpoint'):\n",
        "#             os.mkdir('checkpoint')\n",
        "#         torch.save(state, './checkpoint/ckpt.pth')\n",
        "#         best_acc = acc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwmJ6hWSpuy0"
      },
      "source": [
        "### Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTKUBhizpzv7",
        "outputId": "25d8a8b5-9dfd-48e3-c693-dd99d8f00053"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_losses = []\n",
        "test_accs = []\n",
        "\n",
        "def train(epoch):\n",
        "    global train_losses, train_accs\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    train_losses.append(train_loss/(batch_idx+1))\n",
        "    train_accs.append(100.*correct/total)\n",
        "    print('Train Loss: %.3f | Train Acc: %.3f%% (%d/%d)'% (train_losses[-1], train_accs[-1], correct, total))\n",
        "\n",
        "def test(epoch):\n",
        "    global best_acc, test_losses, test_accs\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "        test_losses.append(test_loss/(batch_idx+1))\n",
        "        test_accs.append(100.*correct/total)\n",
        "        print('Test Loss: %.3f | Test Acc: %.3f%% (%d/%d)'% (test_losses[-1], test_accs[-1], correct, total))\n",
        "\n",
        "        # Save checkpoint.\n",
        "        acc = 100.*correct/total\n",
        "        if acc > best_acc:\n",
        "            print('Saving..')\n",
        "            state = {\n",
        "                'net': net.state_dict(),\n",
        "                'acc': acc,\n",
        "                'epoch': epoch,\n",
        "            }\n",
        "            if not os.path.isdir('checkpoint'):\n",
        "                os.mkdir('checkpoint')\n",
        "            torch.save(state, './checkpoint/ckpt.pth')\n",
        "            best_acc = acc\n",
        "\n",
        "# Train and test the model for multiple epochs\n",
        "for epoch in range(start_epoch, 80):\n",
        "    train(epoch)\n",
        "    test(epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "# Plot the training and testing curves\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(train_losses,label='Train')\n",
        "plt.plot(test_losses,label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(train_accs,label='Train')\n",
        "plt.plot(test_accs,label='Test')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eFWRVKJzvP5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
